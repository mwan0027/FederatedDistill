# Federated Knowledge Distillation PaperList

# About

Reading List of recent published Federated Learning & Knowledge Distillation papers

Reading Priority:
Top: :star2:
Second :star: 

## Recent Federated Knowledge Distillation
<ul>
<li>:star2:Federated Knowledge Distillation (Not published yet)<a href="https://arxiv.org/pdf/2011.02367.pdf">[paper]</a>

<li>:star2:Communication-Efficient Federated Distillation
<a href="https://arxiv.org/pdf/2012.00632">[paper]</a>
</ul>

## Recent FL framework similar to KD
<ul>
<li>:star:Split learning for health: Distributed deep learning without sharing raw patient data <a href="https://arxiv.org/pdf/1812.00564.pdf">[paper]</a>

<li>:star:Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge <a href="https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf">[paper]</a>

<!-- <li>Split learning for collaborative deep learning in healthcare<a href="https://arxiv.org/pdf/1912.12115">[paper]</a>

<li>Splitfed: When federated learning meets split learning<a href="https://arxiv.org/pdf/2004.12088">[paper]</a>

<li>Detailed comparison of communication efficiency of split learning and federated learning <a href="https://arxiv.org/pdf/1909.09145">[paper]</a> -->

</ul>

## FL Surveys
<ul>
<li>:star2:Survey of personalization techniques for federated learning<a href="https://arxiv.org/pdf/2003.08673">[paper]</a>

<li>:star:Advances and Open Problems in Federated Learning<a href="https://arxiv.org/pdf/1912.04977">[paper]</a>


<li>:star:Federated Machine Learning: Concept and Applications <a href="http://sites.nlsde.buaa.edu.cn/~yxtong/tist_fl.pdf">[paper]</a>

<li>From Federated Learning to Federated Neural Architecture Search: A Survey <a href="https://arxiv.org/pdf/2009.05868">[paper]</a>

<li>A survey on security and privacy of federated learning <a href="https://www.sciencedirect.com/science/article/pii/S0167739X20329848">[paper]</a>

<li>Federated Learning: Challenges, Methods, and Future Directions<a href="https://ieeexplore.ieee.org/abstract/document/9084352">[paper]</a>

<li> [IEEE Communications Surveys & Tutorials 2019] Federated learning in mobile edge networks: A comprehensive survey <a href="https://arxiv.org/pdf/1909.11875">[paper]</a>

<li> [IEEE, 2020] Federated learning: A survey on enabling technologies, protocols, and applications <a href="https://ieeexplore.ieee.org/iel7/6287639/8948470/09153560.pdf">[paper]</a>

</ul>


## Federated Learning using Knowledge Distillation

### CV
<ul>

<li>:star:Federated model distillation with noise-free differential privacy<a href="https://arxiv.org/pdf/2009.05537">[paper]</a>

<li>FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning<a href="https://openreview.net/pdf?id=dgtpE6gKjHn">[paper]</a>

<li>:star:FedMD: Heterogenous Federated Learning via Model Distillation<a href="https://arxiv.org/pdf/1910.03581.pdf">[paper]</a><a href="https://github.com/diogenes0319/FedMD_clean">[code]</a>

<li>Federated Learning with Heterogeneous Labels and Models for Mobile Activity Monitoring<a href="https://arxiv.org/pdf/2012.02539">[paper]</a>

<li>:star:Federated Mutual Learning<a href="https://arxiv.org/pdf/2006.16765">[paper]</a>

<li>Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">[paper]</a>

<li>Wireless federated distillation for distributed edge learning with heterogeneous data<a href="https://arxiv.org/pdf/1907.02745">[paper]</a>

<li>Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data<a href="https://arxiv.org/pdf/1811.11479">[paper]</a>

<li>Performance Optimization for Federated Person
Re-identification via Benchmark Analysis<a href="https://arxiv.org/pdf/2008.11560.pdf">[paper]</a> [KD as optimization tool section 5]

<li>KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via
Knowledge Distillation<a href="https://arxiv.org/pdf/2011.09757.pdf">[paper]</a> 

<li>Real-Time Decentralized knowledge Transfer at the Edge<a href="https://arxiv.org/pdf/2011.05961.pdf">[paper]</a> 

</ul>

### NLP

<ul>
<li>:star:FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction<a href="https://www.aclweb.org/anthology/2020.emnlp-main.165.pdf">[paper]</a>
<li>:star:Ensemble distillation for robust model fusion in federated learning<a href="https://papers.nips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Supplemental.pdf">[paper]</a>
<li>:star:Improving question answering by commonsense-based pre-training
<a href="https://arxiv.org/pdf/1809.03568">[paper]</a>
<li>:star:Collaborative fairness in federated learning<a href="https://arxiv.org/pdf/2008.12161">[paper]</a>
<li>CovidNLP: a web application for distilling systemic implications of COVID-19 pandemic with Natural Language processing<a href="https://www.medrxiv.org/content/medrxiv/early/2020/04/29/2020.04.25.20079129.full.pdf">[paper]</a>
<li>Federated learning for healthcare informatics <a href="https://link.springer.com/article/10.1007/s41666-020-00082-4">[paper]</a>

<li>Sentiment detection with FedMD: Federated Learning via Model Distillation<a href="https://www.researchgate.net/profile/Galina_Momcheva/publication/347172951_Sentiment_detection_with_FedMD_Federated_Learning_via_Model_Distillation/links/5fd8731c299bf140880f7e4a/Sentiment-detection-with-FedMD-Federated-Learning-via-Model-Distillation.pdf">[paper]</a>

</ul>


> FL Applications:
<ul>
<li>A review of applications in federated learning<a href="https://www.sciencedirect.com/science/article/pii/S0360835220305532">[paper]</a>
</ul>

